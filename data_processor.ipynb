{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f12f1d4-493c-47e4-ac3d-e943e348533c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The DataProcessor class is designed to process data from a CSV file, i.e., data_4_scaler_label_agg.csv. \n",
    "The key steps include loading data, fitting preprocessing tools, aggregating features, \n",
    "and saving the preprocessing tools and aggregated features as pickle files. \n",
    "These preprocessing tools and aggregated features can then be used for data transformation during inference, \n",
    "whether in stream-based mode or batch processing mode.\n",
    "\n",
    "Some examples of the input CSV:\n",
    "user_id,login_attempts,failed_login_attempts,session_duration,data_transferred,access_sensitive_files,log_text, ... \n",
    "1,5,1,30,100,0,User logged in and accessed files., ... \n",
    "2,3,0,60,200,1,User attempted to login multiple times., ...  \n",
    "3,1,0,90,150,0,User session ended abruptly., ...  \n",
    "1,4,1,45,120,1,User downloaded sensitive files., ...  \n",
    "2,2,1,60,180,1,User session timed out., ...  \n",
    "3,3,0,30,160,0,User transferred data to external device., ...  \n",
    "1,2,1,75,110,0,User session extended., ...\n",
    "'''\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "import joblib\n",
    "import pickle\n",
    "import os\n",
    "import io\n",
    "import sys \n",
    "sys.path.append('/Users/henrychang/sys_security_ai')\n",
    "from utility import get_logger, config_file_loc, load_config, set_working_directory\n",
    "\n",
    "# Set up logging configuration\n",
    "logger = get_logger()\n",
    "\n",
    "class DataProcessor:\n",
    "    def __init__(self, input_file_path, # scaler_label_agg_file_path \n",
    "                missing_dict_file_path,\n",
    "                scaler_file_path,\n",
    "                label_encoders_file_path,\n",
    "                aggregated_features_file_path):\n",
    "        \"\"\"\n",
    "        Initializes the DataProcessor with the input file.\n",
    "\n",
    "        Parameters:\n",
    "            input_file (str): The path to the input CSV file.\n",
    "            missing_dict_file_path (str): The path to the output pickle file.\n",
    "            scaler_file_path (str): The path to the output pickle file.\n",
    "            label_encoders_file_path (str): The path to the output pickle file.\n",
    "            aggregated_features_file_path (str): The path to the output pickle file.\n",
    "        \"\"\"\n",
    "        self.input_file = input_file_path\n",
    "        self.missing_dict_file_path = missing_dict_file_path\n",
    "        self.scaler_file_path = scaler_file_path\n",
    "        self.label_encoders_file_path = label_encoders_file_path\n",
    "        self.aggregated_features_file_path = aggregated_features_file_path\n",
    "        self.df = pd.DataFrame()\n",
    "        self.missing_dict = {}\n",
    "        self.scaler = MinMaxScaler()\n",
    "        self.label_encoders = {}\n",
    "        self.aggregated_features = {}\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Loads data from the specified CSV file into a DataFrame (self.df).\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.df = pd.read_csv(self.input_file)\n",
    "            logger.info(f\"Data loaded from {self.input_file}\")\n",
    "        except FileNotFoundError:\n",
    "            logger.error(f\"Error: File {self.input_file} not found.\")\n",
    "        except pd.errors.EmptyDataError:\n",
    "            logger.error(\"Error: No data in the file.\")\n",
    "        except pd.errors.ParserError:\n",
    "            logger.error(\"Error: Parsing error.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error: {e}\")\n",
    "\n",
    "    def prepare_missing_data(self):\n",
    "        \"\"\"\n",
    "        Prepares missing data based on domain knowledge and saves the missing data dictionary.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Filling NaNs based on data types\n",
    "            for col in self.df.columns:\n",
    "                if self.df[col].dtype == 'float64' or self.df[col].dtype == 'int64':  # Numeric columns\n",
    "                    self.missing_dict[col] = self.df[col].mean()  # Replace with mean (or use median, 0, etc.)\n",
    "                elif self.df[col].dtype == 'object':  # Categorical columns\n",
    "                    self.missing_dict[col] = self.df[col].mode()[0]  # Replace with mode (most frequent value)\n",
    "                elif pd.api.types.is_datetime64_any_dtype(self.df[col]):  # Datetime columns\n",
    "                    self.missing_dict[col] = self.df[col].min()  # Replace with earliest date (or a default date)\n",
    "                elif self.df[col].dtype == 'bool':  # Boolean columns\n",
    "                    self.missing_dict[col] = False  # Replace with False (or True, or majority value)\n",
    "            logger.info(\"Missing data prepared.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during preparing missing data: {e}\")\n",
    "\n",
    "    def fit_scaler(self):\n",
    "        \"\"\"\n",
    "        Fits a MinMaxScaler on the numerical columns of the DataFrame.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            numerical_cols = self.df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "            to_remove = ['user_id']\n",
    "            numerical_cols = [element for element in numerical_cols if element not in to_remove]\n",
    "            self.scaler.fit(self.df[numerical_cols])\n",
    "            logger.info(\"Scaler fitted on numerical columns.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error fitting scaler: {e}\")\n",
    "\n",
    "    def fit_label_encoders(self):\n",
    "        \"\"\"\n",
    "        Fits LabelEncoders on the categorical columns of the DataFrame.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            categorical_cols = self.df.select_dtypes(include=['object']).columns\n",
    "            for col in categorical_cols:\n",
    "                le = LabelEncoder()\n",
    "                le.fit(self.df[col])\n",
    "                self.label_encoders[col] = le\n",
    "            logger.info(\"Label encoders fitted on categorical columns.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error fitting label encoders: {e}\")\n",
    "\n",
    "    def aggregate_features(self):\n",
    "        \"\"\"\n",
    "        Aggregates features by grouping the data by user_id and performing specified aggregations.\n",
    "        Saves the resulting dictionary as an attribute.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Group by 'user_id' and perform the aggregations\n",
    "            aggregated_df = self.df.groupby('user_id').agg(\n",
    "                login_attempts=('login_attempts', 'mean'),\n",
    "                failed_login_attempts=('failed_login_attempts', 'mean'),\n",
    "                session_duration=('session_duration', 'mean'),  # Use 'session_duration': ['mean', 'sum'], if we also need sum.\n",
    "                data_transferred=('data_transferred', 'mean'),\n",
    "                access_sensitive_files=('access_sensitive_files', 'mean'),\n",
    "                count=('user_id', 'size')  # Count the number of occurrences\n",
    "            ).reset_index()  # Reset Index converts user_id back to a regular column.\n",
    "\n",
    "            logger.info(\"Features aggregated\")\n",
    "\n",
    "            # Convert DataFrame to dictionary\n",
    "            aggregated_dict = aggregated_df.set_index('user_id').T.to_dict('dict')\n",
    "\n",
    "            # Save the dictionary to an attribute\n",
    "            self.aggregated_features = aggregated_dict\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during aggregation: {e}\")\n",
    "\n",
    "    def save_preprocessors(self):\n",
    "        \"\"\"\n",
    "        Saves the scaler, label encoders, and aggregated features to .pkl files.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            joblib.dump(self.missing_dict, self.missing_dict_file_path)\n",
    "            joblib.dump(self.scaler, self.scaler_file_path)\n",
    "            joblib.dump(self.label_encoders, self.label_encoders_file_path)\n",
    "            joblib.dump(self.aggregated_features, self.aggregated_features_file_path)\n",
    "            # Using triple quotes ensure that the string can span multiple lines.\n",
    "            logger.info(f\"\"\"Scaler, label encoders, and aggregated features saved to\n",
    "            {self.missing_dict_file_path[:self.missing_dict_file_path.rfind('/') + 1]} directory.\"\"\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving processors: {e}\")\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Runs the data processing pipeline: loads data, prepares missing data,\n",
    "        fits preprocessing tools, aggregates features, and saves the preprocessors.\n",
    "        \"\"\"\n",
    "        self.load_data()\n",
    "        if not self.df.empty:\n",
    "            self.prepare_missing_data()\n",
    "            self.fit_scaler()\n",
    "            self.fit_label_encoders()\n",
    "            self.aggregate_features()\n",
    "            self.save_preprocessors()\n",
    "        else:\n",
    "            logger.info(\"No data to process.\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Load configuration\n",
    "        # config_file_loc = '/Users/henrychang/sys_two_ai/config/config.json'\n",
    "        config = load_config(config_file_loc)\n",
    "        \n",
    "        if config:\n",
    "            # Set desired_directory as working_directory\n",
    "            desired_directory = config.get('desired_directory')\n",
    "            working_directory = set_working_directory(desired_directory)\n",
    "            \n",
    "            if working_directory:\n",
    "                #  # Get input directory of files\n",
    "                input_dir = config.get('input_dir')\n",
    "                input_data_path = os.path.join(working_directory, input_dir)\n",
    "                \n",
    "                # Get path of input data for scaler_label_agg\n",
    "                scaler_label_agg_file = config.get('scaler_label_agg_file')\n",
    "                scaler_label_agg_file_path = os.path.join(input_data_path, scaler_label_agg_file)\n",
    "\n",
    "                # Get output directory of files\n",
    "                output_dir = config.get('output_dir')\n",
    "                output_data_path = os.path.join(working_directory, output_dir)\n",
    "\n",
    "                # Get path of output dictionary for missing_dict\n",
    "                missing_dict_file = config.get('missing_dict')\n",
    "                missing_dict_file_path = os.path.join(output_data_path, missing_dict_file)\n",
    "\n",
    "                # Get path of output scaler pickle file\n",
    "                scaler_file = config.get('scaler')\n",
    "                scaler_file_path = os.path.join(output_data_path, scaler_file)\n",
    "\n",
    "                # Get path of output label_encoders pickle file\n",
    "                label_encoders_file = config.get('label_encoders')\n",
    "                label_encoders_file_path = os.path.join(output_data_path, label_encoders_file)\n",
    "\n",
    "                # Get path of output aggregated_features pickle file\n",
    "                aggregated_features_file = config.get('aggregated_features')\n",
    "                aggregated_features_file_path = os.path.join(output_data_path, aggregated_features_file)\n",
    "                \n",
    "                preprocessor = DataProcessor(scaler_label_agg_file_path, \n",
    "                                             missing_dict_file_path,\n",
    "                                             scaler_file_path,\n",
    "                                             label_encoders_file_path,\n",
    "                                             aggregated_features_file_path\n",
    "                                            )\n",
    "                                             \n",
    "                preprocessor.run()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error in main execution: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe8f639-dedf-4e68-8aff-4a70e07734af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbab8202-e602-469e-8bd5-20f77e11f949",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef480ae5-f2d5-43f1-be1c-ea96fe288f30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c2d18a-4bad-456f-85ec-e602ead514e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
