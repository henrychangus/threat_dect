{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ccec4d-de53-4e8a-b628-2746179727cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-18 19:15:15,702 - INFO - Path for configuration file: /Users/henrychang/sys_two_ai/config/config.json\n",
      "2025-03-18 19:15:15,703 - INFO - Configuration file loaded successfully\n",
      "2025-03-18 19:15:15,703 - INFO - Current Working Directory: /Users/henrychang/sys_two_ai\n",
      "2025-03-18 19:15:15,731 - INFO - Using MPS\n",
      "Device set to use mps:0\n",
      "2025-03-18 19:15:18,206 - INFO - Missing data replacement dictionary loaded successfully\n",
      "2025-03-18 19:15:18,214 - INFO - Trained Isolation Forest model loaded.\n",
      "2025-03-18 19:15:18,215 - INFO - Vectorizer, scaler, and label encoders loaded.\n",
      "2025-03-18 19:15:18,215 - INFO - Aggregated features dictionary loaded.\n",
      "2025-03-18 19:15:18,218 - INFO - Simple filtering applied\n",
      "2025-03-18 19:15:18,219 - INFO - Data preprocessed\n",
      "2025-03-18 19:15:18,224 - INFO - Anomalies detected and scores assigned\n",
      "2025-03-18 19:15:18,225 - INFO - Anomaly data post-processed\n",
      "2025-03-18 19:15:18,225 - INFO - Filtered to likely anomalies only\n",
      "2025-03-18 19:15:18,225 - INFO - Features categorized\n",
      "2025-03-18 19:15:18,227 - INFO - Features normalized\n",
      "2025-03-18 19:15:18,227 - INFO - Using MPS\n",
      "Device set to use mps:0\n",
      "2025-03-18 19:15:20,021 - INFO - Logs classified with LLM\n",
      "2025-03-18 19:15:20,024 - INFO - Aggregated features merged with chunk\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "AI Agent for Data Transformation:\n",
    "Combining traditional machine learning (ML) techniques with modern Large Language Models (LLMs) \n",
    "enhances the detection and analysis of security threats in log data and other types of data \n",
    "such as network session data, connection information, traffic flow data, process execution metadata, \n",
    "registry access data, file data, malware signatures, exploits, attacker infrastructure \n",
    "(domains and IP addresses), geographic locations of threat actors or incidents, unusual patterns \n",
    "in network traffic or system behavior, and more.\n",
    "\n",
    "Hybrid Approach:\n",
    "    Traditional ML Techniques: \n",
    "        Methods like Isolation Forests, clustering algorithms, and statistical analysis identify \n",
    "    patterns and outliers in structured data.\n",
    "    Large Language Models: \n",
    "        Advanced LLMs (e.g., GPT-3.5 Turbo, GPT-4, or their fine-tuned versions) excel at parsing \n",
    "        and interpreting complex log entries, generating contextual insights and responses.\n",
    "        \n",
    "Data Processing:\n",
    "    Stream-based Approach:\n",
    "        Data are read in and processed sequentially. For example, by our proprietary algorithm, \n",
    "        the aggregation features can be prepared efficiently in real-time.\n",
    "    Filtering Non-Threat Data: \n",
    "        Non-relevant data is filtered out at the beginning to reduce costs.\n",
    "    Data Augmentation: \n",
    "        Augmentation is provided as needed for better predictive performance.\n",
    "        \n",
    "By integrating these approaches, the AI agent enhances threat detection accuracy through \n",
    "a combination of numerical and contextual analysis.\n",
    "'''\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "import openai\n",
    "import json\n",
    "import joblib\n",
    "import requests\n",
    "\n",
    "import os\n",
    "# os.environ['PYTHONPATH'] = '/Users/henrychang/Library/Python/3.9/lib/python/site-packages' \n",
    "import sys \n",
    "sys.path.append('/Users/henrychang/sys_security_ai')\n",
    "from utility import get_logger, config_file_loc, load_config, set_working_directory, check_and_set_device\n",
    "\n",
    "# Set up logging configuration\n",
    "# import logging\n",
    "logger = get_logger()\n",
    "\n",
    "class CybersecurityDataTransformation:\n",
    "    \"\"\"\n",
    "    A class designed to detect cybersecurity threats using various machine learning models and \n",
    "    AI data processing techniques. The detection results can be treated as enriched features. \n",
    "    Combined with other features, these results can be used for further classification or \n",
    "    detection tasks.\n",
    " \n",
    "    Attributes:\n",
    "        api_key (str): AI model API key.\n",
    "        input_file (str): Path to the input file.\n",
    "        output_file (str): Path to the output file.\n",
    "        vectorizer_file (str): Path to the vectorizer file.\n",
    "        model_file (str): Path to the model file.\n",
    "        scaler_file (str): Path to the scaler file.\n",
    "        label_encoders_file (str): Path to the label encoders file.\n",
    "        aggregated_features_file (str): Path to the aggregated features file.\n",
    "        fine_tune_id_file (str): Path to the fine-tuned model ID file.\n",
    "        chunk_size (int): Size of data chunks to process. Default is 1000.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 api_key,\n",
    "                 input_file, \n",
    "                 output_file, \n",
    "                 missing_dict_file, \n",
    "                 vectorizer_file, \n",
    "                 model_file, \n",
    "                 scaler_file, \n",
    "                 label_encoders_file, \n",
    "                 aggregated_features_file, \n",
    "                 fine_tune_id_file, \n",
    "                 chunk_size=1000):\n",
    "        self.input_file = input_file\n",
    "        self.output_file = output_file\n",
    "        self.missing_dict_file = missing_dict_file\n",
    "        self.vectorizer_file = vectorizer_file\n",
    "        self.model_file = model_file\n",
    "        self.scaler_file = scaler_file\n",
    "        self.label_encoders_file = label_encoders_file\n",
    "        self.aggregated_features_file = aggregated_features_file\n",
    "        self.fine_tune_id_file = fine_tune_id_file\n",
    "        self.chunk_size = chunk_size\n",
    "        self.df = pd.DataFrame()\n",
    "        self.missing_dict = {}\n",
    "        self.vectorizer = CountVectorizer()\n",
    "        self.isolation_forest = IsolationForest()\n",
    "        self.scaler = MinMaxScaler()\n",
    "        self.label_encoders = {}\n",
    "        self.aggregated_features = {}\n",
    "        # Create OpenAI client by the OpenAI API key\n",
    "        self.client = openai.OpenAI(api_key=api_key)\n",
    "        # If using the Hugging Face transformers library and the models are available locally or \n",
    "        # downloaded from the Hugging Face model hub, we do not need an API key.\n",
    "        # Initialize the generative model. If resources are limited, use DistilGPT-2; \n",
    "        # otherwise, use GPT-4. GPT-3.5 Turbo is a balanced option.\n",
    "        device = check_and_set_device()\n",
    "        self.text_generator = pipeline('text-generation', model='gpt2-large', max_length=50, truncation=True, device=device) \n",
    "        # May use model='gpt-3.5-turbo' if it's available locally or supported by transformers pipeline.\n",
    "        # truncation ensures the input text is truncated to fit within the model's input limits.\n",
    "        # For example, GPT-2, typically has a maximum input length of 1,024 tokens.\n",
    "        # max_length controls the maximum length of the output text (in tokens).\n",
    "\n",
    "    def load_fine_tuned_model(self):\n",
    "        \"\"\"\n",
    "        Loads the fine-tuned model using the saved fine_tune_id.\n",
    "\n",
    "        Returns:\n",
    "            str: The fine-tuned model identifier.\n",
    "\n",
    "        Raises:\n",
    "            SystemExit: If an error occurs while loading the fine-tuned model.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Read the fine-tune ID from the file\n",
    "            with open(self.fine_tune_id_file, 'r') as f:\n",
    "                fine_tune_id = f.read().strip()\n",
    "            logger.info(f\"Fine-tune ID read from file: {fine_tune_id}\")\n",
    "\n",
    "            # Retrieve the fine-tune response using the fine-tune ID\n",
    "            fine_tune_response = self.client.FineTune.retrieve(id=fine_tune_id)\n",
    "            fine_tuned_model = fine_tune_response['fine_tuned_model']\n",
    "            logger.info(f\"Fine-tuned model loaded: {fine_tuned_model}\")\n",
    "\n",
    "            return fine_tuned_model\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading fine-tuned model: {e}\")\n",
    "            sys.exit(1)  # Exit the program with status code 1 indicating an error\n",
    "\n",
    "\n",
    "    def load_preprocessors(self):\n",
    "        \"\"\"\n",
    "        Loads the vectorizer, scaler, and label encoders from their respective files.\n",
    "\n",
    "        Raises:\n",
    "            SystemExit: If an error occurs while loading any of the preprocessors.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.vectorizer = joblib.load(self.vectorizer_file)\n",
    "            self.scaler = joblib.load(self.scaler_file)\n",
    "            self.label_encoders = joblib.load(self.label_encoders_file)\n",
    "            logger.info(\"Vectorizer, scaler, and label encoders loaded.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading preprocessors: {e}\")\n",
    "            sys.exit(1)  # Exit the program with status code 1 indicating an error\n",
    "\n",
    "    def load_isolation_forest(self):\n",
    "        \"\"\"\n",
    "        Loads the trained Isolation Forest model from the model file.\n",
    "\n",
    "        Raises:\n",
    "            SystemExit: If an error occurs while loading the Isolation Forest model.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.isolation_forest = joblib.load(self.model_file)\n",
    "            logger.info(\"Trained Isolation Forest model loaded.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading trained Isolation Forest model: {e}\")\n",
    "            sys.exit(1)  # Exit the program with status code 1 indicating an error\n",
    "\n",
    "    def load_aggregated_features(self):\n",
    "        \"\"\"\n",
    "        Loads the aggregated features dictionary from the aggregated features file.\n",
    "\n",
    "        Raises:\n",
    "            SystemExit: If an error occurs while loading the aggregated features dictionary.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.aggregated_features = joblib.load(self.aggregated_features_file)\n",
    "            logger.info(\"Aggregated features dictionary loaded.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading aggregated features dictionary: {e}\")\n",
    "            sys.exit(1)  # Exit the program with status code 1 indicating an error\n",
    "\n",
    "\n",
    "    def load_missing_data_replacement(self):\n",
    "        \"\"\"\n",
    "        Loads the dictionary containing precomputed values for missing data.\n",
    "        Raises:\n",
    "            SystemExit: If an error occurs while loading the missing values dictionary.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load the dictionary containing precomputed values for missing data using joblib \n",
    "            self.missing_dict = joblib.load(self.missing_dict_file) \n",
    "            logger.info(\"Missing data replacement dictionary loaded successfully\")\n",
    "        except FileNotFoundError:\n",
    "            logger.error(f\"File not found: {self.missing_dict_file}\")\n",
    "            sys.exit(1)  # Exit the program with status code 1 indicating an error\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading file: {e}\")\n",
    "            sys.exit(1)  # Exit the program with status code 1 indicating an error\n",
    "\n",
    "    '''\n",
    "    Key Points to Consider\n",
    "    Data Distribution: Choose the imputation method based on the distribution of the data.\n",
    "    Impact on Analysis: Consider how the imputed values might affect subsequent analyses and model performance.\n",
    "    Consistency: Ensure that the imputation method is applied consistently across similar datasets.\n",
    "    '''\n",
    "    def preprocess_data(self, chunk):\n",
    "        \"\"\"\n",
    "        Preprocesses the data chunk by filling missing values using precomputed values from a dictionary.\n",
    "        Parameters:\n",
    "            chunk (DataFrame): The data chunk to preprocess.\n",
    "        Returns:\n",
    "            DataFrame: The preprocessed data chunk.\n",
    "        Raises:\n",
    "            SystemExit: If an error occurs during preprocessing.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Iterate through each column in the chunk to fill missing values\n",
    "            for col in chunk.columns:\n",
    "                # Numeric columns, Categorical columns, Boolean columns\n",
    "                if chunk[col].dtype in ['float64', 'int64', 'object', 'bool']:\n",
    "                    chunk[col].fillna(self.missing_dict[col])  # Use precomputed value\n",
    "                elif pd.api.types.is_datetime64_any_dtype(chunk[col]):  # Datetime columns\n",
    "                    chunk[col].fillna(self.missing_dict[col])  # Use precomputed value\n",
    "            logger.info(\"Data preprocessed\")\n",
    "            return chunk\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during preprocessing: {e}\")\n",
    "            sys.exit(1)  # Exit the program if preprocessing fails\n",
    "\n",
    "\n",
    "    '''\n",
    "    Remove rows that are obviously not threats.\n",
    "    '''\n",
    "    def simple_filter(self, chunk):\n",
    "        \"\"\"\n",
    "        Applies a simple rule-based filter to remove non-threat rows based on certain conditions.\n",
    "        Parameters:\n",
    "            chunk (DataFrame): The data chunk to process.\n",
    "        Returns:\n",
    "            DataFrame: The filtered data chunk.\n",
    "        Raises:\n",
    "            None: Logs any exceptions encountered and returns the original chunk.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Define conditions to filter out non-threat rows \n",
    "            filter_conditions = [ 'login successful', \n",
    "                                 'system update', \n",
    "                                 'user logged in', \n",
    "                                 'user logged out', \n",
    "                                 'system rebooted', \n",
    "                                 'file saved', \n",
    "                                 'file opened', \n",
    "                                 'file closed', \n",
    "                                 'session ended', \n",
    "                                 'session started', \n",
    "                                 'heartbeat message', \n",
    "                                 'backup completed', \n",
    "                                 'scheduled task completed', \n",
    "                                 'configuration updated' ]\n",
    "\n",
    "            # Apply the filter conditions to the chunk\n",
    "            chunk = chunk[~chunk['log_text'].str.contains('|'.join(filter_conditions), case=False)]\n",
    "            logger.info(\"Simple filtering applied\")\n",
    "            return chunk\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during simple filtering: {e}\")\n",
    "            return chunk\n",
    "\n",
    "    def vectorize_data(self, chunk):\n",
    "        \"\"\"\n",
    "        Vectorizes the text data in the given data chunk using the pre-trained vectorizer.\n",
    "        Parameters:\n",
    "            chunk (DataFrame): The data chunk to process.\n",
    "        Returns:\n",
    "            sparse matrix: The vectorized text data.\n",
    "        Raises:\n",
    "            None: Logs any exceptions encountered and returns None.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Extract text data from the chunk\n",
    "            text_data = chunk['log_text']\n",
    "\n",
    "            # Transform the text data using the pre-trained vectorizer\n",
    "            vectorized_data = self.vectorizer.transform(text_data)\n",
    "            return vectorized_data\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during vectorization: {e}\")\n",
    "            return None\n",
    "\n",
    "    \n",
    "    '''\n",
    "    The exact range of these scores depends on the data and the\n",
    "    specifics of the Isolation Forest model, but they often fall\n",
    "    within a range approximately between -1 and 1, where:\n",
    "\n",
    "    Scores closer to 0: Indicate points that are somewhat\n",
    "    isolated but not extreme.\n",
    "    Strong negative scores (e.g., below -0.5): Indicate points\n",
    "    that are strongly considered as anomalies.\n",
    "    Scores approaching 1: Indicate points that are considered\n",
    "    normal.\n",
    "    '''\n",
    "    def predict_anomalies(self, chunk, vectorized_data):\n",
    "        \"\"\"\n",
    "        Detects anomalies in the given data chunk using the trained Isolation Forest model.\n",
    "        Parameters:\n",
    "            chunk (DataFrame): The data chunk to process.\n",
    "            vectorized_data (sparse matrix): The vectorized data corresponding to the chunk.\n",
    "        Returns:\n",
    "            DataFrame: The data chunk with anomaly predictions and scores.\n",
    "        Raises:\n",
    "            None: Logs any exceptions encountered and returns None if an error occurs.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Predict anomalies using the Isolation Forest model\n",
    "            predictions = self.isolation_forest.predict(vectorized_data)\n",
    "            chunk['anomaly'] = predictions\n",
    "\n",
    "            # Calculate anomaly scores (derived from the number of splits required to isolate a data point)\n",
    "            anomaly_scores = self.isolation_forest.decision_function(vectorized_data)\n",
    "            chunk['anomaly_score'] = anomaly_scores\n",
    "            logger.info(\"Anomalies detected and scores assigned\")\n",
    "            return chunk\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during anomaly detection: {e}\")\n",
    "            return None\n",
    "\n",
    "            \n",
    "    def postprocess_anomalies(self, chunk):\n",
    "        \"\"\"\n",
    "        Post-processes the anomaly predictions by assigning human-readable labels.\n",
    "        Parameters:\n",
    "            chunk (DataFrame): The data chunk to process.\n",
    "        Returns:\n",
    "            DataFrame: The data chunk with anomaly labels.\n",
    "        Raises:\n",
    "            None: Logs any exceptions encountered and returns None if an error occurs.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Assign human-readable labels based on anomaly predictions\n",
    "            chunk['anomaly_label'] = chunk['anomaly'].apply(lambda x: 'Anomaly' if x == -1 else 'Normal')\n",
    "            logger.info(\"Anomaly data post-processed\")\n",
    "            return chunk\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during anomaly post-processing: {e}\")\n",
    "            return None\n",
    "\n",
    "    '''\n",
    "    Remove rows that are obviously not threats.\n",
    "\n",
    "    In practice, the threshold (x) is usually set to 0. So:\n",
    "    Anomaly Score < 0: Likely an anomaly.\n",
    "    Anomaly Score ≥ 0: Likely not an anomaly (normal).\n",
    "\n",
    "    Using a more stringent threshold like 0.5 instead of 0.0 can help in\n",
    "           identifying anomalies with greater caution.\n",
    "    Anomaly Score < 0.5: Likely an anomaly.\n",
    "    Anomaly Score ≥ 0.5: Likely not an anomaly (normal).\n",
    "    '''\n",
    "    def filter_anomalies(self, chunk):\n",
    "        \"\"\"\n",
    "        Filters the data chunk to include only likely anomalies based on the anomaly score.\n",
    "        Parameters:\n",
    "            chunk (DataFrame): The data chunk to process.\n",
    "        Returns:\n",
    "            DataFrame: The filtered data chunk containing only likely anomalies.\n",
    "        Raises:\n",
    "            None: Logs any exceptions encountered and returns the original chunk if an error occurs.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Filter the chunk to include rows with anomaly scores less than 0.5\n",
    "            chunk = chunk[chunk['anomaly_score'] < 0.5]\n",
    "            logger.info(\"Filtered to likely anomalies only\")\n",
    "            return chunk\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during anomaly filtering: {e}\")\n",
    "            return None\n",
    "\n",
    "    def categorize_features(self, chunk):\n",
    "        \"\"\"\n",
    "        Encodes categorical features in the data chunk using pre-trained label encoders.\n",
    "        Parameters:\n",
    "            chunk (DataFrame): The data chunk to process.\n",
    "        Returns:\n",
    "            DataFrame: The data chunk with encoded categorical features.\n",
    "        Raises:\n",
    "            None: Logs any exceptions encountered and returns None if an error occurs.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Select categorical columns for encoding\n",
    "            categorical_cols = chunk.select_dtypes(include=['object']).columns.tolist()\n",
    "            # Elements to remove: \n",
    "            # log_text column will be processed differently.\n",
    "            # anomaly_label column has been represented by anomaly column.\n",
    "            to_remove = ['log_text', 'anomaly_label']\n",
    "            # Use a list comprehension to filter out the elements \n",
    "            categorical_cols = [element for element in categorical_cols if element not in to_remove]\n",
    "\n",
    "            # Apply label encoders to transform categorical features\n",
    "            for col in categorical_cols:\n",
    "                chunk[col] = self.label_encoders[col].transform(chunk[col])\n",
    "            logger.info(\"Features categorized\")\n",
    "            return chunk\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during categorization: {e}\")\n",
    "            return None\n",
    "            \n",
    "    def normalize_features(self, chunk):\n",
    "        \"\"\"\n",
    "        Normalizes numerical features in the data chunk using the pre-trained scaler.\n",
    "        Parameters:\n",
    "            chunk (DataFrame): The data chunk to process.\n",
    "        Returns:\n",
    "            DataFrame: The data chunk with normalized numerical features.\n",
    "        Raises:\n",
    "            None: Logs any exceptions encountered and returns None if an error occurs.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Select numerical columns for normalization\n",
    "            numerical_cols = chunk.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "            to_remove = ['user_id']\n",
    "            # Use a list comprehension to filter out the elements \n",
    "            numerical_cols = [element for element in numerical_cols if element not in to_remove]\n",
    "\n",
    "            # Apply the scaler to normalize numerical features\n",
    "            chunk[numerical_cols] = self.scaler.transform(chunk[numerical_cols])\n",
    "            logger.info(\"Features normalized\")\n",
    "            return chunk\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during normalization: {e}\")\n",
    "            return None\n",
    "    \n",
    "    '''\n",
    "    model='distilbert-base-uncased-finetuned-sst-2-english': \n",
    "    Specifies the pre-trained model to use. In this case, it's a DistilBERT model fine-tuned \n",
    "    on the SST-2 dataset, which is commonly used for sentiment analysis \n",
    "    (e.g., classifying text as positive or negative).\n",
    "    '''\n",
    "    def classify_logs_with_llm(self, chunk):\n",
    "        \"\"\"\n",
    "        Classifies log entries in the given data chunk using a pre-trained language model (LLM).\n",
    "        Parameters:\n",
    "            chunk (DataFrame): The data chunk to process.\n",
    "        Returns:\n",
    "            DataFrame: The data chunk with classified log entries.\n",
    "        Raises:\n",
    "            None: Logs any exceptions encountered and returns None.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            device = check_and_set_device()\n",
    "            \n",
    "            # Initialize the text classification pipeline with a pre-trained model\n",
    "            classifier = pipeline('text-classification', model='distilbert-base-uncased-finetuned-sst-2-english', device=device)\n",
    "\n",
    "            # Apply the classifier to each log entry and store the classification result\n",
    "            # 'POSITIVE': Indicates that the text has a positive sentiment. \n",
    "            #             For example, \"User login successful for user 'admin' from IP address 192.168.1.100.\"\n",
    "            # 'NEGATIVE': Indicates that the text has a negative sentiment.\n",
    "            #             For example, \"Multiple failed login attempts detected for user 'admin' from \n",
    "            #                           IP address 192.168.1.100. Possible brute-force attack.\"\n",
    "            chunk['classification'] = chunk['log_text'].apply(lambda x: classifier(x)[0]['label'])\n",
    "\n",
    "            logger.info(\"Logs classified with LLM\")\n",
    "            return chunk\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during classification: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "    '''\n",
    "    Example of the dictionary:\n",
    "    {\n",
    "        'user_1': {\n",
    "            'count': 7,\n",
    "            'login_attempts': 3.6666666666666665, \n",
    "            'failed_login_attempts': 1.0,\n",
    "            'session_duration': 50.0,\n",
    "            'data_transferred': 110.0,\n",
    "            'access_sensitive_files': 0.3333333333333333, 'count': 3 \n",
    "        },\n",
    "        'user_2': {\n",
    "        ...\n",
    "        },\n",
    "      ...\n",
    "    }\n",
    "    \n",
    "    columns of chunk after aggregate_features(chunk):\n",
    "    user_id  login_attempts  failed_login_attempts  session_duration  data_transferred  access_sensitive_files  count  login_attempts_agg  failed_login_attempts_agg  session_duration_agg  data_transferred_agg  access_sensitive_files_agg\n",
    "    '''\n",
    "    def aggregate_features(self, chunk):\n",
    "        \"\"\" \n",
    "        Aggregates features from the given data chunk and updates the aggregated features dictionary. \n",
    "        Parameters: \n",
    "            chunk (DataFrame): The data chunk to process. \n",
    "        Returns: \n",
    "            DataFrame: The data chunk merged with the aggregated features. \n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Iterate through each row in the chunk\n",
    "            for index, row in chunk.iterrows():\n",
    "                user_id = row['user_id']\n",
    "                if user_id not in self.aggregated_features:\n",
    "                    # Initialize the aggregated features for a new user_id\n",
    "                    self.aggregated_features[user_id] = {\n",
    "                        'count': 1,\n",
    "                        'login_attempts': row['login_attempts'],\n",
    "                        'failed_login_attempts': row['failed_login_attempts'],\n",
    "                        'session_duration': row['session_duration'],\n",
    "                        'data_transferred': row['data_transferred'],\n",
    "                        'access_sensitive_files': row['access_sensitive_files']\n",
    "                    }\n",
    "                else:\n",
    "                    # Update the aggregated features for an existing user_id\n",
    "                    self.aggregated_features[user_id]['count'] += 1\n",
    "                    for feature in ['login_attempts', 'failed_login_attempts', 'session_duration', 'data_transferred', 'access_sensitive_files']:\n",
    "                        self.aggregated_features[user_id][feature] += (1.0 / self.aggregated_features[user_id]['count']) * (row[feature] - self.aggregated_features[user_id][feature])\n",
    "           \n",
    "            # Convert dictionary to DataFrame\n",
    "            aggregated_df = pd.DataFrame.from_dict(self.aggregated_features, orient='index').reset_index().rename(columns={'index': 'user_id'})\n",
    "           \n",
    "            # Merge aggregated data back with the original chunk\n",
    "            # Perform a left join to ensure the row count of `chunk` remains the same\n",
    "            chunk = pd.merge(chunk, aggregated_df, on='user_id', how='left', suffixes=('', '_agg'))\n",
    "           \n",
    "            logger.info(\"Aggregated features merged with chunk\")\n",
    "            return chunk\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during aggregation: {e}\")\n",
    "            return None\n",
    "    \n",
    "    '''\n",
    "    This is done to enrich the training dataset and improve the performance of machine learning models, \n",
    "    such as those used for further classification tasks.\n",
    "    '''\n",
    "    def augment_data(self, chunk):\n",
    "        \"\"\" \n",
    "        Augments the data in the given chunk by paraphrasing text and modifying numerical values. \n",
    "        Parameters: \n",
    "            chunk (DataFrame): The data chunk to process. \n",
    "        Returns: \n",
    "            DataFrame: The augmented data chunk combined with the original chunk. \n",
    "        \"\"\"\n",
    "        try:\n",
    "            def paraphrase_text(row, num_return_sequences=2):\n",
    "                \"\"\" \n",
    "                Paraphrases the text in the log entry using a text generation model. \n",
    "                Parameters: \n",
    "                    row (Series): A row of data containing the log text. \n",
    "                    num_return_sequences (int): The number of paraphrased sequences to generate. \n",
    "                Returns: \n",
    "                    list: A list of paraphrased text sequences. \n",
    "                \"\"\"\n",
    "                try:\n",
    "                    # Generate paraphrases\n",
    "                    paraphrases = self.text_generator(row['log_text'], \n",
    "                                                      max_length=50, \n",
    "                                                      num_return_sequences=num_return_sequences) \n",
    "                    return [p['generated_text'] for p in paraphrases]\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error during paraphrasing: {e}\")\n",
    "                    return [row['log_text']]  # Return original text if paraphrasing fails\n",
    "    \n",
    "            def augment_numerical(value):\n",
    "                \"\"\" \n",
    "                Augments a numerical value by multiplying it with a random factor. \n",
    "                Parameters: \n",
    "                    value (float or int): The numerical value to augment. \n",
    "                Returns: \n",
    "                    float or int: The augmented numerical value. \n",
    "                \"\"\"\n",
    "                try:\n",
    "                    augmented_value = value * np.random.uniform(0.9, 1.1)\n",
    "                    if isinstance(value, np.int64):\n",
    "                        return int(round(augmented_value))\n",
    "                    else:\n",
    "                        return augmented_value\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error during numerical augmentation: {e}\")\n",
    "                    return value  # Return original value if augmentation fails\n",
    "    \n",
    "            augmented_rows = []\n",
    "            for index, row in chunk.iterrows():\n",
    "                try:\n",
    "                    # Paraphrase the log text\n",
    "                    paraphrased_logs = paraphrase_text(row, num_return_sequences=2)\n",
    "    \n",
    "                    for paraphrased_log in paraphrased_logs:\n",
    "                        new_row = row.copy()\n",
    "    \n",
    "                        # Set new paraphrased log text\n",
    "                        new_row['log_text'] = paraphrased_log\n",
    "    \n",
    "                        # Augment numerical columns\n",
    "                        for col in chunk.select_dtypes(include=['float64', 'int64']).columns:\n",
    "                            if col == 'user_id':\n",
    "                                continue\n",
    "                            new_row[col] = augment_numerical(new_row[col])\n",
    "    \n",
    "                        augmented_rows.append(new_row)\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error processing row {index}: {e}\")\n",
    "                    continue\n",
    "            augmented_chunk = pd.DataFrame(augmented_rows)\n",
    "            return pd.concat([chunk, augmented_chunk], ignore_index=True)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error augmenting data: {e}\")\n",
    "            return chunk  # Return original chunk if augmentation fails\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Using row['log_text'] as Query:\n",
    "    \n",
    "    The detect_threat function uses row['log_text'] as the query to retrieve relevant documents.\n",
    "    \n",
    "    Document Retrieval:\n",
    "    The retrieve_documents method uses the query to search for related documents via the Bing Search API.\n",
    "    A typical JSON response from the Bing Search API might look like this:\n",
    "    {\n",
    "    \"webPages\": {\n",
    "        \"value\": [\n",
    "            {\n",
    "                \"name\": \"Example Web Page 1\",\n",
    "                \"url\": \"https://www.example.com/page1\",\n",
    "                \"snippet\": \"This is an example snippet for web page 1.\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"Example Web Page 2\",\n",
    "                \"url\": \"https://www.example.com/page2\",\n",
    "                \"snippet\": \"This is an example snippet for web page 2.\"\n",
    "            },\n",
    "            // More results...\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    \n",
    "    Augmented Prompt:\n",
    "    \n",
    "    The retrieved documents are included as additional context in the prompt.\n",
    "    '''\n",
    "    def retrieve_documents(self, query):\n",
    "        \"\"\" \n",
    "        Retrieves relevant documents for the given query using the Bing Search API. \n",
    "        Parameters: \n",
    "            query (str): The search query text. \n",
    "        Returns: \n",
    "            list: A list of retrieved document snippets relevant to the query. \n",
    "        Raises: \n",
    "            None: Logs any exceptions encountered and returns None. \n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Formulate the search query dynamically based on the log entry text\n",
    "            search_query = f\"System Two Security: {query}\"\n",
    "           \n",
    "            # Make a request to the Bing Search API\n",
    "            response = requests.get(\n",
    "                'https://api.bing.microsoft.com/v7.0/search',\n",
    "                params={'q': search_query, 'count': 5},\n",
    "                headers={'Ocp-Apim-Subscription-Key': 'THE_BING_SEARCH_API_KEY'} # Replace THE_BING_SEARCH_API_KEY with the actual Bing Search API key.\n",
    "            )\n",
    "            \n",
    "            # Parse the JSON response\n",
    "            search_results = response.json()\n",
    "           \n",
    "            # Extract relevant documents (simplified for illustration)\n",
    "            retrieved_docs = [result['snippet'] for result in search_results.get('webPages', {}).get('value', [])]\n",
    "\n",
    "            return retrieved_docs\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error retrieving documents for query '{query}': {e}\")\n",
    "            return None\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Primary Focus on Log Entry: The prompt focuses on the original log entry and its attributes.\n",
    "    \n",
    "    Additional Context: The retrieved documents are added as supplementary context, not the main focus.\n",
    "    \n",
    "    Balanced Prompt: This ensures that the decision-making process revolves around the log entry while \n",
    "    benefiting from additional context.\n",
    "\n",
    "    For example, given a prompt like:\n",
    "        Log entry: Unauthorized access attempt detected\n",
    "        Sentiment Classification: Negative\n",
    "        Predictive Anomaly Label: High\n",
    "        Is this potentially a threat?\n",
    "        Additional context: Multiple failed login attempts from an unknown IP address\n",
    "    The generated_text might be something like:\n",
    "        \"This is likely a threat based on the high anomaly label and multiple failed login attempts.\"\n",
    "\n",
    "    Please note that all the features already created can be used to prepare a sophisticated prompt for more \n",
    "    accurate threat prediction. The prompt in detect_threats() is only a simple example.\n",
    "    '''\n",
    "    def detect_threats(self, chunk):\n",
    "        \"\"\"\n",
    "        Detects threats in the given data chunk using a generative AI model.\n",
    "        Parameters:\n",
    "            chunk (DataFrame): The data chunk to process.\n",
    "        Returns:\n",
    "            DataFrame: The processed data chunk with threat detection results.\n",
    "        \"\"\"\n",
    "        def detect_threat(row):\n",
    "            \"\"\"\n",
    "            Detects threats for a single row of data.\n",
    "            Parameters:\n",
    "                row (Series): A row of data from the chunk.\n",
    "            Returns:\n",
    "                str: 'Threat' if the generated text indicates a threat, 'Normal' otherwise.\n",
    "            \"\"\"\n",
    "            try:\n",
    "                # Use the log entry text as the query\n",
    "                query = row['log_text']\n",
    "                \n",
    "                # Retrieve relevant documents for the log entry\n",
    "                retrieved_docs = self.retrieve_documents(query)\n",
    "                docs_text = \"\\n\".join(retrieved_docs)\n",
    "                \n",
    "                # Create a prompt for threat detection\n",
    "                prompt = (\n",
    "                    f\"Log entry: {row['log_text']}\\n\"\n",
    "                    f\"Sentiment Classification: {row['classification']}\\n\"\n",
    "                    f\"Predictive Anomaly Label: {row['anomaly_label']}\\n\"\n",
    "                    \"Is this a threat?\\n\"\n",
    "                    f\"Additional context:\\n{docs_text}\"\n",
    "                )\n",
    "\n",
    "                # Generate text using text_generator as the threat detection model\n",
    "                generated_text = self.text_generator(prompt, \n",
    "                                                             max_length=250, \n",
    "                                                             num_return_sequences=1)[0]['generated_text']\n",
    "                # The max_length=250 specified here will take precedence and override the default setting \n",
    "                # in pipeline initialization.\n",
    "                \n",
    "                return 'Threat' if 'threat' in generated_text.lower() else 'Normal'\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error during threat detection for row: {row.name} - {e}\")\n",
    "                return 'Error'\n",
    "    \n",
    "        try:\n",
    "            # Apply the detect_threat function to each row in the chunk\n",
    "            chunk['threat_detection'] = chunk.apply(detect_threat, axis=1)\n",
    "            logger.info(\"Threats detected using generative AI\")\n",
    "            return chunk\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error applying threat detection: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def save_chunk(self, chunk):\n",
    "        \"\"\"\n",
    "        Saves the processed data chunk to the output file.\n",
    "        Parameters:\n",
    "            chunk (DataFrame): The data chunk to save.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            chunk.to_csv(self.output_file, \n",
    "                         mode='a', \n",
    "                         index=False, \n",
    "                         header=not pd.io.common.file_exists(self.output_file))\n",
    "            logger.info(f\"Chunk saved to {self.output_file}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving chunk: {e}\")\n",
    "    \n",
    "    def save_aggregated_features(self):\n",
    "        \"\"\"\n",
    "        Saves the aggregated features to a file.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            joblib.dump(self.aggregated_features, self.aggregated_features_file)\n",
    "            logger.info(\"Aggregated features saved.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving aggregated features: {e}\")\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\" \n",
    "        Runs the data processing pipeline in chunks. This method loads the preprocessors, aggregated features, etc. \n",
    "        It then processes the input data in chunks, applying various \n",
    "        data transformations and saving results. \n",
    "        \"\"\"\n",
    "        self.load_missing_data_replacement()\n",
    "        self.load_isolation_forest()\n",
    "        self.load_preprocessors()\n",
    "        self.load_aggregated_features()\n",
    "        # If we need to fine-tune GPT-3.5-Turbo on the specific dataset using OpenAI’s fine-tuning API. \n",
    "        # Once fine-tuned, save fine_tune_id locally and upload it to get fine_tuned_model.\n",
    "        # Additionally, we may implement rag_text_generation(query) to use RAG for better predictive performance.\n",
    "        '''\n",
    "        fine_tuned_model = self.load_fine_tuned_model()\n",
    "        device = check_and_set_device()\n",
    "        self.text_generator = pipeline('text-generation', model=fine_tuned_model, device=device)\n",
    "        '''\n",
    "        try:\n",
    "            chunks = pd.read_csv(self.input_file, chunksize=self.chunk_size)\n",
    "            for chunk in chunks:\n",
    "                try:\n",
    "                    if (chunk := self.simple_filter(chunk)) is None:\n",
    "                        continue\n",
    "                    if (chunk := self.preprocess_data(chunk)) is None:\n",
    "                        continue\n",
    "                    if (vectorized_data := self.vectorize_data(chunk)) is None:\n",
    "                        continue\n",
    "                    if (chunk := self.predict_anomalies(chunk, vectorized_data)) is None:\n",
    "                        continue\n",
    "                    if (chunk := self.postprocess_anomalies(chunk)) is None:\n",
    "                        continue\n",
    "                    if (chunk := self.filter_anomalies(chunk)) is None:\n",
    "                        continue\n",
    "                    if (chunk := self.categorize_features(chunk)) is None:\n",
    "                        continue\n",
    "                    if (chunk := self.normalize_features(chunk)) is None:\n",
    "                        continue\n",
    "                    if (chunk := self.classify_logs_with_llm(chunk)) is None:\n",
    "                        continue\n",
    "                    if (chunk := self.aggregate_features(chunk)) is None:\n",
    "                        continue\n",
    "                    if (chunk := self.augment_data(chunk)) is None:\n",
    "                        continue\n",
    "                    if (chunk := self.detect_threats(chunk)) is None:\n",
    "                        continue\n",
    "                    self.save_chunk(chunk)\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error processing chunk: {e}\")\n",
    "            self.save_aggregated_features()  # Ensure to call this method outside the inner try-except block\n",
    "        except FileNotFoundError:\n",
    "            logger.error(f\"Error: File not found.\")\n",
    "        except pd.errors.EmptyDataError:\n",
    "            logger.error(\"Error: No data in the file.\")\n",
    "        except pd.errors.ParserError:\n",
    "            logger.error(\"Error: Parsing error.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading data in chunks: {e}\")\n",
    "\n",
    "\n",
    "class CustomError(Exception):\n",
    "    \"\"\"\n",
    "    This approach ensures that errors are properly logged and propagated without interfering with the IPython or \n",
    "    Jupyter Notebook environment's error handling mechanisms.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Load configuration\n",
    "        config = load_config(config_file_loc)\n",
    "        \n",
    "        if config:\n",
    "            # Set desired_directory as working_directory\n",
    "            desired_directory = config.get('desired_directory')\n",
    "            working_directory = set_working_directory(desired_directory)\n",
    "\n",
    "            # Get API key\n",
    "            api_key = config.get('api_key')\n",
    "            \n",
    "            if working_directory:\n",
    "                # Get input directory of files\n",
    "                input_dir = config.get('input_dir')\n",
    "                input_data_path = os.path.join(working_directory, input_dir)\n",
    "\n",
    "                # Get path of input data for transform \n",
    "                input_data_4_transform_file = config.get('input_data_4_transform')\n",
    "                input_data_4_transform_file_path = os.path.join(input_data_path, input_data_4_transform_file)\n",
    "                \n",
    "                # Get output directory of files\n",
    "                output_dir = config.get('output_dir')\n",
    "                output_data_path = os.path.join(working_directory, output_dir)\n",
    "\n",
    "                # Get path of output dictionary for missing_dict\n",
    "                transform_results_file = config.get('transform_results')\n",
    "                transform_results_file_path = os.path.join(output_data_path, transform_results_file)\n",
    "                \n",
    "                # Get path of output dictionary for missing_dict\n",
    "                missing_dict_file = config.get('missing_dict')\n",
    "                missing_dict_file_path = os.path.join(output_data_path, missing_dict_file)\n",
    "\n",
    "                # Get path of output scaler pickle file\n",
    "                scaler_file = config.get('scaler')\n",
    "                scaler_file_path = os.path.join(output_data_path, scaler_file)\n",
    "\n",
    "                # Get path of output label_encoders pickle file\n",
    "                label_encoders_file = config.get('label_encoders')\n",
    "                label_encoders_file_path = os.path.join(output_data_path, label_encoders_file)\n",
    "\n",
    "                # Get path of output aggregated_features pickle file\n",
    "                aggregated_features_file = config.get('aggregated_features')\n",
    "                aggregated_features_file_path = os.path.join(output_data_path, aggregated_features_file)\n",
    "\n",
    "                # Get output file path\n",
    "                fine_tune_id_file = config.get('fine_tune_id_file')\n",
    "                fine_tune_id_path = os.path.join(output_data_path, fine_tune_id_file)\n",
    "\n",
    "                # Get path of output vectorizer pickle file\n",
    "                vectorizer_file = config.get('vectorizer')\n",
    "                vectorizer_file_path = os.path.join(output_data_path, vectorizer_file)\n",
    "\n",
    "                #  Get path of output vectorizer pickle file\n",
    "                isolation_forest_model_file = config.get('isolation_forest_model')\n",
    "                isolation_forest_model_file_path = os.path.join(output_data_path, isolation_forest_model_file)\n",
    "\n",
    "                detector = CybersecurityDataTransformation(\n",
    "                    api_key,\n",
    "                    input_data_4_transform_file_path, \n",
    "                    transform_results_file_path, \n",
    "                    missing_dict_file_path, \n",
    "                    vectorizer_file_path, \n",
    "                    isolation_forest_model_file_path, \n",
    "                    scaler_file_path, \n",
    "                    label_encoders_file_path, \n",
    "                    aggregated_features_file_path, \n",
    "                    fine_tune_id_path,\n",
    "                    chunk_size=1000\n",
    "                )\n",
    "\n",
    "                try:\n",
    "                    detector.run()\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error executing run method: {e}\")\n",
    "                    raise CustomError(f\"Error executing run method: {e}\") from e\n",
    "    except FileNotFoundError as fnf_error:\n",
    "        logger.error(f\"Configuration file not found: {fnf_error}\")\n",
    "        raise CustomError(f\"Configuration file not found: {fnf_error}\") from fnf_error\n",
    "    except json.JSONDecodeError as json_error:\n",
    "        logger.error(f\"Error decoding JSON from the configuration file: {json_error}\")\n",
    "        raise CustomError(f\"Error decoding JSON from the configuration file: {json_error}\") from json_error\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error in main execution: {e}\")\n",
    "        raise CustomError(f\"Unexpected error in main execution: {e}\") from e\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e6d075-2eff-48e9-bf58-cbcb57f74247",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6568bcc7-0969-49bf-92ba-b3e48d149c7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
